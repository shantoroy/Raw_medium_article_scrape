{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import operator\n",
    "import string\n",
    "\n",
    "\n",
    "# Class to append value in a dictionary\n",
    "class MyDictionary(dict):\n",
    "\n",
    "    # __init__ function\n",
    "    def __init__(self):\n",
    "        self = dict()\n",
    "\n",
    "    # Function to add key:value\n",
    "    def add(self, key, value):\n",
    "        self[key] = value\n",
    "\n",
    "\n",
    "class Relevancy(object):\n",
    "    def __init__(self, file_name, tag):\n",
    "        self.tag = tag\n",
    "        self.file_name = file_name\n",
    "        with open(self.file_name, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    # Python3 code to pre-processing the string\n",
    "    # Intending to remove all punctuation and common words\n",
    "    def pre_process(self, str):\n",
    "        # removing punctuation ---> using string module\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        str = str.translate(translator)\n",
    "\n",
    "        # converting all into lower cases\n",
    "        str = str.lower()\n",
    "        \n",
    "        # removing numbers and digits\n",
    "        # str = ''.join([i for i in str if not i.isdigit()])\n",
    "        regex = re.compile('[^a-zA-Z]')\n",
    "        #First parameter is the replacement, second parameter is your input string\n",
    "        str = regex.sub(' ', str)\n",
    "\n",
    "        # removing prep, conj, articles ---> using re module\n",
    "        str = re.sub('(\\s+)(a|an|and|the|this|that|these|those|i|would|could|should|m|ve)(\\s+)', ' ', str)\n",
    "        str = re.sub('(\\s+)(to|for|from|in|into|under|with|within|below|up|down|of|on|s|t)(\\s+)', ' ', str)\n",
    "        str = re.sub('(\\s+)(are|may|by|as|we|or|it|be|which|the|when|make|no|set|your|its|it\\'s)(\\s+)', ' ', str)\n",
    "        str = re.sub('(\\s+)(if|any|used|all|has|have|new|data|at|code|node|state|-|they|our)(\\s+)', ' ', str)\n",
    "        str = re.sub('(\\s+)(you|must|every|each|not|what|one|then|way|so|will|also|is|can|\"|\")(\\s+)', ' ', str)\n",
    "        str = re.sub('(\\s+)(their|was|more|other|use|do|need|my|some|get|out|many|had|here|over)(\\s+)', ' ', str)\n",
    "        return str\n",
    "\n",
    "    # Python3 code to find frequency of each word\n",
    "    # function for calculating the frequency\n",
    "    def freq(self, str):\n",
    "        str = self.pre_process(str)\n",
    "\n",
    "        # break the string into list of words\n",
    "        str_list = str.split()\n",
    "\n",
    "        # gives set of unique words\n",
    "        unique_words = set(str_list)\n",
    "        frequency = MyDictionary()\n",
    "        for word in unique_words:\n",
    "            frequency.add(word, str_list.count(word))\n",
    "\n",
    "        # sort by value (downwards) ---> using operator module\n",
    "        sorted_freq = sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        # collect first 10 items in dictionary\n",
    "        # first_few_items = {k: sorted_freq[k] for k in list(sorted_freq)[:10]}\n",
    "        # btw, sorted_freq is a list\n",
    "        return sorted_freq[0:20]  # , len(sorted_freq)\n",
    "\n",
    "    # merge two lists toether without appending same word\n",
    "    # if same word found, increases count\n",
    "    def merge_two_lists(self, a, b):\n",
    "        new_list = {}\n",
    "        for pair in a + b:\n",
    "            key, value = pair\n",
    "            new_list[key] = new_list.get(key, 0) + value\n",
    "        new_list = [[key, value] for key, value in new_list.items()]\n",
    "        return new_list\n",
    "\n",
    "    # convert a tuple into a list\n",
    "    def tuple_to_list(self, listname):\n",
    "        a = []\n",
    "        for i in range(0, len(listname)):\n",
    "            a.append(list(listname[i]))\n",
    "        return a\n",
    "\n",
    "    # For sorting a list of lists using 2nd item\n",
    "    def second_item(self, item):\n",
    "        return item[1]\n",
    "\n",
    "    # for getting a list of words for a particular tag\n",
    "    def list_of_most_used_words(self):\n",
    "        freq_list = []\n",
    "        for key in self.data:\n",
    "            if self.tag in key['title'].lower():\n",
    "                str = key['content']\n",
    "                freq_list = self.merge_two_lists(freq_list, self.tuple_to_list(self.freq(str)))\n",
    "\n",
    "        freq_list.sort(key=self.second_item, reverse=True)\n",
    "        reduced_list = []\n",
    "        for i in freq_list:\n",
    "            if i[1] > 1000:\n",
    "                reduced_list.append(i)\n",
    "        return reduced_list\n",
    "\n",
    "    # Return number of Relevant posts\n",
    "    def relevant_post(self):\n",
    "        relevant = 0\n",
    "        for key in self.data:\n",
    "            if self.tag in key['title'].lower():\n",
    "                relevant += 1\n",
    "        return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Relevant post is:373\n",
      "[['ethereum', 4558], ['blockchain', 1641], ['contract', 1259], ['but', 1057]]\n"
     ]
    }
   ],
   "source": [
    "# application code\n",
    "if __name__ == \"__main__\":\n",
    "    file_name = \"ethereum.json\"\n",
    "    tag = 'ethereum'\n",
    "    keyword = Relevancy(file_name, tag)\n",
    "    print(\"Number of Relevant post is:\" + str(keyword.relevant_post()))\n",
    "    print(keyword.list_of_most_used_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevancy Search in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475\n",
      "485\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from math import floor\n",
    "count = 0\n",
    "file_name = \"data/ethereum.json\"\n",
    "with open(file_name, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "relevant = 0\n",
    "for key in data:\n",
    "    if 'ethereum' in key['title'].lower():\n",
    "        relevant += 1\n",
    "for key in data:\n",
    "    count += 1      \n",
    "p = floor(relevant/count * 100)\n",
    "print(relevant)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better relevancy search using finding tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482\n",
      "485\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from math import floor\n",
    "count = 0\n",
    "file_name = \"data/ethereum.json\"\n",
    "with open(file_name, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "relevant = 0\n",
    "for key in data:\n",
    "    count += 1\n",
    "    tags = key['tags']\n",
    "    for item in tags: \n",
    "        if 'ethereum' in item.lower():\n",
    "            relevant += 1\n",
    "print(relevant)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input all relevant posts into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "final_count = 0\n",
    "final_json_data = []\n",
    "\n",
    "file_name_list = [\"ethereum.json\", \"blockchain.json\", \"smart-contract.json\",\n",
    "                      \"solidity.json\", \"vyper.json\", \"ripple.json\",\n",
    "                      \"remix.json\", \"metamask.json\", \"bitcoin.json\"]\n",
    "tag_list = [\"ethereum\", \"blockchain\", \"smart contract\", \"solidity\", \"vyper\", \"ripple\",\n",
    "                \"remix\", \"metamask\", \"bitcoin\"]\n",
    "\n",
    "for file_name, tag in zip(file_name_list, tag_list):\n",
    "\n",
    "    json_data  = json.load(open(filename))\n",
    "\n",
    "    for key in json_data:\n",
    "        tags = key['tags']\n",
    "        for item in tags: \n",
    "            if tag in item.lower():\n",
    "                final_json_data.append(key)\n",
    "                count += 1\n",
    "    print(\"Number of total post for \", tag, \"is =\", count)\n",
    "\n",
    "    # Output the updated file with pretty JSON                                      \n",
    "open(\"final_all_post_data.json\", \"w\").write(\n",
    "        json.dumps(final_json_data, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    )\n",
    "print(\"The number of total post is: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"ename\": [\"mark\", \"egg\"], \"url\": \"Lennon.com\"}, {\"ename\": [\"egg\"], \"url\": \"Lennon.com\"}]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "data  = json.load(open(\"_test_.json\"))\n",
    "new = []\n",
    "\n",
    "for key in data:\n",
    "    count += 1\n",
    "    tags = key['ename']\n",
    "    for item in tags: \n",
    "        if 'egg' in item.lower():\n",
    "            new.append(key)\n",
    "\n",
    "new = json.dumps(new)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tag = {\"ethereum-390.json\":\"ethereum\", \"vyper-272.json\":\"vyper\",\n",
    "                \"bitcoin-281.json\":\"bitcoin\", \"smart-contract-369\":\"smart contract\",\n",
    "                \"blockchain-351.json\":\"blockchain\", \"solidity-374\":\"solidity\"}\n",
    "for x,y in file_tag.items():\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2019': 5,\n",
       " '2018': 3,\n",
       " '2017': 1,\n",
       " '2016': 2,\n",
       " '2015': 1,\n",
       " '2014': 0,\n",
       " 'missing': 2}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#year_list = ['2019', '2018', '2017', '2016', '2015', '2014']\n",
    "#month_list = ['January', 'February', 'March', 'April', 'May', 'June', 'July',\n",
    "            # 'August', 'September', 'October', 'November', 'December']\n",
    "freq = {\"2019\":0, \"2018\":0, \"2017\":0, \"2016\":0, \"2015\":0, \"2014\":0, \"missing\":0}\n",
    "date_list = ['27 April, 2018', '17 December', '12 May, 2017', '17 January',\n",
    "            '16 June, 2018', '16 April', '', '16 April', '11 May, 2016', '', '17 June, 2018',\n",
    "             '11 May, 2016', '27 April, 2015', '16 April']\n",
    "for date in date_list:\n",
    "    if ',' in date:\n",
    "        y = date.split(',')[1]\n",
    "        y = y.lstrip()\n",
    "        freq[y] += 1\n",
    "    elif date=='':\n",
    "        freq[\"missing\"] += 1\n",
    "    else:\n",
    "        freq[\"2019\"] += 1\n",
    "        \n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016', '11', '27']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '2016-11-27'\n",
    "a = a.split(\"-\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panda DataFrame for Medium Posts\n",
    "file_name = \"metamask-350.json\"\n",
    "dates = []\n",
    "with open(file_name, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(data, orient='columns')\n",
    "#df[['post_date', 'upvotes']]\n",
    "for key in data:\n",
    "    dates.append(key['post_date'])\n",
    "if dates[26]=='':\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Name': 'Bala', 'phone': 'None'}, {'Name': 'Bala1', 'phone': 'None'}]\n"
     ]
    }
   ],
   "source": [
    "te = [\n",
    "      {\n",
    "        \"Name\": \"Bala\",\n",
    "        \"phone\": \"None\"\n",
    "      },\n",
    "      {\n",
    "        \"Name\": \"Bala\",\n",
    "        \"phone\": \"None\"\n",
    "      },\n",
    "      {\n",
    "        \"Name\": \"Bala\",\n",
    "        \"phone\": \"None\"\n",
    "      },\n",
    "      {\n",
    "        \"Name\": \"Bala\",\n",
    "        \"phone\": \"None\"\n",
    "      },\n",
    "      {\n",
    "          \"Name\": \"Bala1\",\n",
    "          \"phone\": \"None\"\n",
    "      }      \n",
    "    ]\n",
    "\n",
    "unique = { each['Name'] : each for each in te }\n",
    "a = list(unique.values())\n",
    "with open(\"test_3.json\",\"w\") as fp:\n",
    "    json.dump(a, fp)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of post after removing duplicay =  2156\n"
     ]
    }
   ],
   "source": [
    "with open(\"final_all_post_data.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "unique_post = {each['title'] : each for each in data}\n",
    "listed_dictionary_items = list(unique_post.values())\n",
    "with open(\"final_data_removing_duplicacy.json\",\"w\") as f:\n",
    "    json.dump(listed_dictionary_items, f)\n",
    "print(\"# of post after removing duplicay = \", len(listed_dictionary_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associative Tag analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5755\n",
      "980\n",
      "Counter({'Smart Contracts': 1783, 'Ethereum': 1417, 'Blockchain': 1128, 'Solidity': 769, 'Cryptocurrency': 229, 'Truffle': 211, 'Bitcoin': 154, 'Tutorial': 144, 'Programming': 138, 'Dapps': 118, 'ICO': 109, 'Security': 99, 'Web3': 90, 'Erc20': 69, 'Development': 65, 'Technology': 55, 'Crypto': 53, 'Blockchain Development': 50, 'Neo': 44, 'Vyper': 44, 'JavaScript': 43, 'Token': 34, 'Tech': 30, 'Smart Contracts Tutorial': 30, 'Open Source': 25, 'Decentralization': 25, 'Blockchain Technology': 25, '이더리움': 25, '솔리디티': 25, 'Testing': 23, 'Token Sale': 20, 'Hacking': 20, 'Openzeppelin': 20, 'Gochain': 20, 'Software Development': 20, 'Audit': 19, 'Metamask': 18, 'Programming Languages': 15, 'Ethereum Blockchain': 15, 'Partnerships': 15, 'Developer Tools': 15, 'Guide': 15, 'Startup': 15, 'Altcoins': 15, 'Real Estate': 15, 'Solidity Tutorial': 15, 'Vulnerability': 15, 'Smart Contract Security': 14, 'Rsk': 14, 'Dispute Resolutions': 14, 'Remix': 14, 'Oracle': 10, 'Coding': 10, 'Python': 10, 'Stellar': 10, 'Law': 10, 'Contracts': 10, 'Legaltech': 10, 'English': 10, 'Governance': 10, 'Crowdsale': 10, 'IoT': 10, 'Cybersecurity': 10, 'Visual Studio': 10, 'Ether': 10, 'Erc721': 10, 'Zeppelin': 10, 'Evm': 10, 'Formal Verification': 10, 'Solidity 102': 10, 'Mathematics': 10, 'Ganache': 10, 'Rsksmart': 9, 'Qtum': 9, 'Distributed Ledgers': 9, 'Eos': 9, 'Eosio': 7, 'Nodejs': 7, 'Future Of Work': 5, '區塊鏈': 5, '智能合約': 5, 'Reentrancy Attack': 5, 'Ethereum Smart Contracts': 5, 'Python Smart Contracts': 5, 'Upgradable': 5, 'Upgradeable': 5, 'Product Management': 5, 'Prototyping': 5, 'Iota': 5, 'Qubic': 5, 'Java': 5, 'Geth': 5, 'Deploy Smart Contracts': 5, 'Bitcoincash': 5, 'Smart Contract Platforms': 5, 'Contractpedia': 5, 'Hello World': 5, 'Honeypot': 5, 'Stellar Lumens': 5, 'Innovation': 5, 'Functional Programming': 5, 'Dapp Developers': 5, 'Dapp Development': 5, 'Ethereum Development': 5, 'Cryptocurrencies': 5, 'Steampunk': 5, 'Language Design': 5, 'Smart Cities': 5, 'Private Blockchain': 5, 'Marketplaces': 5, 'Proof Of Stake': 5, 'Simplicity': 5, 'Justice': 5, 'What Are Smart Contracts': 5, 'Eip 999': 5, 'Insurance': 5, 'Brickblock': 5, 'Content Creation': 5, 'Myetherwallet': 5, 'Bef': 5, 'Whitelist': 5, 'Evolive': 5, 'Evopro': 5, 'Ideafex': 5, 'Wallet': 5, 'Mywish': 5, 'Etherparty': 5, 'Appcoins': 5, 'Comparison': 5, 'API': 5, 'Hackernoon Top Story': 5, 'Software Engineering': 5, 'Custody': 5, 'Videocoin': 5, 'Videos': 5, 'Social Networks': 5, 'Legal': 5, 'Aeternity': 5, 'Team': 5, 'Aepps': 5, 'News': 5, 'Kadena': 5, 'Blockchain Smart Contract': 5, 'Smart Contract Pitfalls': 5, 'Decentralized': 5, 'Case Study': 5, 'Bug Bounty': 5, 'Chain Link': 5, 'Crytpocurrencies': 5, 'Public Blockchain': 5, 'Research': 5, 'Columbia University': 5, 'Crowdfunding': 5, 'Smart Contract Auditing': 5, 'Pchain': 5, 'Stable Coin': 5, 'Environmental Issues': 5, 'Democracy': 5, 'Voting': 5, 'Blockchain Público': 5, 'Inmobilliaria': 5, 'Dapp': 5, 'The Dao': 5, 'Tor': 5, 'Censorship': 5, 'Investing': 5, 'Trading': 5, 'Gifto': 5, 'Uplive': 5, 'Asia Innovations Group': 5, 'Finance': 5, 'Fintech': 5, 'Slack': 5, 'Yee Token': 5, 'Yee': 5, 'Yeecall': 5, 'Technews': 5, 'Content Marketing': 5, 'Token Economy': 5, 'Assets': 5, 'Ganache Cli': 5, 'Libraries': 5, 'Interview Questions': 5, 'Public Functions': 5, 'External Functions': 5, 'Best Practices': 5, 'Versioned Contracts': 5, 'Upgradable Contracts': 5, 'Ethereum Solidity': 5, 'Guides And Tutorials': 5, 'Game Theory': 5, 'Database': 5, 'Guides': 5, 'Typescript': 5, 'Plasma': 5, 'Scalability': 5, 'Bugs': 5, 'Kvm': 5, 'Iulia': 5, 'Lll': 5, 'Testrpc': 5, 'Learning To Code': 5, 'T7': 5, 'NPM': 5, 'Sevilla': 5, 'Proptech': 5, 'Reverse Engineering': 5, 'Defi': 5, 'Data Storage': 5, 'Data Structures': 5, 'Bankers Rounding': 5, 'Dsl': 5, 'Domain Specific Languages': 5, 'Gas': 5, 'Golang': 5, 'Solidity Tutorials': 5, 'Formal Analysis': 5, 'Continuous Integration': 5, 'Windows 10': 5, 'Metacoin': 5, 'Uniswap': 5, 'Interfaces': 5, 'Ivy': 5, 'Ide': 5, 'Developer': 5, 'Smart Contacts Tutorial': 5, 'Infura': 5, 'React': 5, 'Debugging': 5, 'Automated Testing': 5, 'Deployment': 5, 'Online Courses': 5, 'Test Driven Development': 5, 'Chromebook': 5, 'Linux': 5, 'Blockchain Programming': 5, 'GraphQL': 5, 'Zeppelinos': 5, 'Adex': 5, 'Neo Tokens': 5, 'Citowise': 5, 'Bitnautic': 5, 'Xrt': 5, 'Github': 4, 'Ternion': 4, 'Operating Systems': 4, 'Truth': 4, 'Universa': 4, 'Universa Education': 4, 'Zerion': 4, 'Beginner': 4, 'Islamic Finance': 4, 'Flint': 4, 'Technical': 4, 'Legislation': 4, 'Student Voices': 4, 'Scratchcard': 4, 'Lottery': 4, 'Cheatsheet': 4, 'Ethers': 4, 'Escrow': 4, 'Foodtech': 4, 'Gaming': 4, 'Erc': 4, 'Thekey': 3, 'Virtual Machine': 3, 'Unit Testing': 3})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../conference_medium_data/author_data/related_data_rm_duplicacy.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "count = 0\n",
    "tag_list = []\n",
    "for key in data:\n",
    "    tag_list.append([i for i in key['tags']])\n",
    "    count += 1\n",
    "\n",
    "# 2 ways to remove nested lists\n",
    "from itertools import chain\n",
    "break_nested_list = list(chain(*tag_list))\n",
    "print(len(break_nested_list))\n",
    "# flattened  = [val for sublist in list_of_lists for val in sublist]\n",
    "unique_list_of_tags = list(set(break_nested_list))\n",
    "print(len(unique_list_of_tags))\n",
    "\n",
    "from collections import Counter\n",
    "associated_with_tag = list(set([item for items in tag_list if \"Solidity\" in items for item in items]))\n",
    "top_tags_for_tag = Counter([item for items in tag_list if \"Solidity\" in items for item in items])\n",
    "\n",
    "top_tags_only = Counter([item for items in tag_list for item in items])\n",
    "\n",
    "associated_with_smart_contract = list(set([item for items in tag_list if \"Smart Contracts\"\\\n",
    "                                           in items for item in items or \"Smart Contract\" in items for item in items]))\n",
    "top_tags_for_smart_contracts = Counter(([item for items in tag_list if \"Smart Contracts\"\\\n",
    "                                           in items for item in items or \"Smart Contract\" in items for item in items]))\n",
    "#print(associated_with_tag)\n",
    "print(top_tags_for_smart_contracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag frequency count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solidity = 419\n",
      "ethereum = 856\n",
      "blockchain = 708\n",
      "web3 = 217\n",
      "metamask = 192\n",
      "truffle = 140\n",
      "remix = 5\n",
      "token = 32\n",
      "tokens = 1\n",
      "erc20 = 138\n",
      "vyper = 26\n",
      "smart contract = 3\n",
      "smart contracts = 377\n",
      "ethereum blockchain = 30\n",
      "myetherwallet = 11\n",
      "wallet = 18\n",
      "dapp = 6\n",
      "dapps = 100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../conference_medium_data/author_data/related_data_rm_duplicacy.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "tag_list = [\"solidity\", \"ethereum\", \"blockchain\", \"web3\", \"metamask\", \"truffle\", \"remix\", \"token\",\\\n",
    "            \"tokens\", \"erc20\", \"vyper\", \"smart contract\", \"smart contracts\", \"ethereum blockchain\",\\\n",
    "            \"myetherwallet\", \"wallet\", \"dapp\", \"dapps\",]\n",
    "\n",
    "for i in tag_list:\n",
    "    count = 0\n",
    "    for key in data:\n",
    "        if i in [item.lower() for item in key['tags']]:\n",
    "            count += 1\n",
    "            # print(key['title'])\n",
    "    print(i, \"=\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4682\n",
      "Average=  3.7819063004846525\n",
      "Counter({0: 619, 1: 205, 2: 105, 3: 68, 4: 42, 5: 41, 7: 25, 8: 21, 6: 19, 9: 18, 11: 13, 10: 8, 12: 7, 14: 5, 21: 4, 17: 3, 23: 3, 13: 3, 16: 3, 18: 2, 39: 2, 153: 1, 68: 1, 24: 1, 19: 1, 22: 1, 79: 1, 20: 1, 29: 1, 40: 1, 72: 1, 165: 1, 105: 1, 63: 1, 131: 1, 684: 1, 57: 1, 26: 1, 136: 1, 15: 1, 124: 1, 128: 1, 129: 1})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../conference_medium_data/author_data/related_data_rm_duplicacy.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "response_list = []\n",
    "for key in data:\n",
    "    response_list.append(key['responses'])\n",
    "    count += 1\n",
    "response_freq = Counter([item for item in response_list])\n",
    "print(sum(response_list))\n",
    "print(\"Average= \", sum(response_list)/len(response_list))\n",
    "print(response_freq)\n",
    "# print(sorted(response_list, reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg claps and voters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max clap: 35690\n",
      "avg clap: 823.3447432762836\n",
      "max voter: 5027\n",
      "avg voter: 73.19070904645477\n",
      "claps per voter: 11.24930683146818\n",
      "total claps: 673496\n",
      "total voters: 59870\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "with open(\"../conference_medium_data/author_data/related_data_rm_duplicacy.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "clap_list = []\n",
    "voter_list = []\n",
    "data_dict = dict()\n",
    "for key in data:\n",
    "    clap_list.append(key['claps'])\n",
    "    voter_list.append(key['voters'])\n",
    "for i,j in zip(clap_list,voter_list):\n",
    "    data_dict[i] = j\n",
    "\n",
    "new_clap = []\n",
    "new_voter = []\n",
    "for key in data:\n",
    "    for i in key['tags']:\n",
    "        if \"Blockchain\" in i or \"Tokens\" in i:\n",
    "            new_clap.append(key['claps'])\n",
    "            new_voter.append(key['voters'])\n",
    "            \n",
    "print(\"max clap:\", max(new_clap))\n",
    "print(\"avg clap:\", sum(new_clap)/len(new_clap))\n",
    "print(\"max voter:\", max(new_voter))\n",
    "print(\"avg voter:\", sum(new_voter)/len(new_voter))\n",
    "print(\"claps per voter:\", sum(new_clap)/sum(new_voter))\n",
    "print(\"total claps:\", sum(new_clap))\n",
    "print(\"total voters:\", sum(new_voter))\n",
    "# print(sorted(new_clap, reverse=True))\n",
    "# print(sorted(new_voter, reverse=True))\n",
    "# response_freq = Counter([item for item in response_list])\n",
    "# print(voter_list)\n",
    "# print(\"Average= \", sum(response_list)/len(response_list))\n",
    "# print(clap_list)\n",
    "# print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vulnerability Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "security = 328\n",
      "vulnerability = 49\n",
      "vulnerabilities = 73\n",
      "reentrancy = 15\n",
      "re entrancy = 0\n",
      "re-entrancy = 7\n",
      "race condition = 6\n",
      "denial of service = 7\n",
      "DoS = 13\n",
      "transaction order = 8\n",
      "transactions order = 0\n",
      "trasaction order depend = 0\n",
      "transaction_ordering_depend = 0\n",
      "timestamp dependence = 0\n",
      "integer overflow = 14\n",
      "integer underflow = 2\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../conference_medium_data/author_data/related_data_rm_duplicacy.json\") as f:\n",
    "    post_data = json.load(f)\n",
    "\n",
    "word_list = [\"security\", \"vulnerability\", \"vulnerabilities\", \"reentrancy\", \"re entrancy\", \"re-entrancy\", \"race condition\",\\\n",
    "             \"denial of service\", \"DoS\", \"transaction order\", \"transactions order\", \"trasaction order depend\",\\\n",
    "             \"transaction_ordering_depend\", \"timestamp dependence\", \"integer overflow\", \"integer underflow\"]\n",
    "# word_list = [\"transaction order\"]\n",
    "for i in word_list:\n",
    "    count = 0\n",
    "    for key in post_data:\n",
    "        if i in key['content'] or i in key['title'].lower() or i in key['tags']:\n",
    "            count += 1\n",
    "            # print(key['title'])\n",
    "    print(i, \"=\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mythril = 0\n",
      "mythx = 4\n",
      "mythos = 0\n",
      "oyente = 0\n",
      "solhint = 1\n",
      "solium = 0\n",
      "ethlint = 0\n",
      "securify = 0\n",
      "teether = 0\n",
      "smartcheck = 1\n",
      "manticore = 0\n",
      "sonarsolidity = 0\n",
      "ethir = 0\n",
      "maian = 0\n",
      "solcheck = 0\n",
      "solgraph = 0\n",
      "solint = 0\n",
      "vandal = 0\n",
      "contractfuzzer = 0\n",
      "rattle = 0\n",
      "sasc = 0\n",
      "zeus = 0\n",
      "contractlarva = 0\n",
      "echinda = 0\n",
      "ethertrust = 0\n",
      "fsolidm = 0\n",
      "octopus = 0\n",
      "osiris = 0\n",
      "reguard = 0\n",
      "scompile = 0\n",
      "slither = 1\n",
      "surya = 0\n",
      "sūrya = 0\n",
      "verisolid = 0\n",
      "verx = 0\n",
      "vultron = 0\n"
     ]
    }
   ],
   "source": [
    "sec_tool_list = [\"mythril\", \"mythx\", \"mythos\", \"oyente\", \"solhint\", \"solium\", \"ethlint\",\\\n",
    "                 \"securify\", \"teether\", \"smartcheck\", \"manticore\", \"sonarsolidity\", \"ethir\",\\\n",
    "                 \"maian\", \"solcheck\", \"solgraph\", \"solint\", \"vandal\", \"contractfuzzer\",\\\n",
    "                 \"rattle\", \"sasc\", \"zeus\", \"contractlarva\", \"echinda\", \"ethertrust\", \"fsolidm\",\\\n",
    "                 \"octopus\", \"osiris\", \"reguard\", \"scompile\", \"slither\", \"surya\", \"sūrya\", \"verisolid\",\\\n",
    "                 \"verx\", \"vultron\"]\n",
    "# sec_tool_list = [\"sūrya\"]\n",
    "for i in sec_tool_list:\n",
    "    count = 0\n",
    "    for key in post_data:\n",
    "        if (i in key['content'].lower() or i in key['title'].lower() or i in key['tags']) \\\n",
    "                            and (\"transaction ordering\" in key['content'].lower() or i in key['title'].lower()):\n",
    "                                 #or (\"timestamp-depend\" in key['content'].lower() or i in key['title'].lower())):\n",
    "            count += 1\n",
    "            # print(key['title'])\n",
    "    print(i, \"=\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of mentions of popular tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solidity = 1697\n",
      "goethereum = 0\n",
      "go-ethereum = 10\n",
      "web3 = 886\n",
      "web3js = 17\n",
      "contract development = 0\n",
      "blockchain = 2303\n",
      "ethereum = 3389\n",
      "truffle = 1790\n",
      "transaction = 1460\n",
      "transactions = 725\n",
      "remix = 194\n",
      "contract design = 0\n",
      "token = 2578\n",
      "tokens = 1894\n",
      "ether = 835\n",
      "erc20 = 693\n",
      "erc-20 = 195\n",
      "metamask = 1524\n",
      "mining = 133\n",
      "mine = 53\n",
      "javascript = 346\n",
      "private blockchain = 0\n",
      "wallet = 1057\n",
      "wallets = 188\n",
      "gas = 1079\n",
      "parity = 156\n",
      "parities = 0\n"
     ]
    }
   ],
   "source": [
    "tags = [\"solidity\", \"goethereum\", \"go-ethereum\", \"web3\", \"web3js\", \"contract development\", \"blockchain\", \"ethereum\", \"truffle\",\\\n",
    "        \"transaction\", \"transactions\", \"remix\", \"contract design\", \"token\", \"tokens\", \"ether\", \"erc20\", \"erc-20\",\\\n",
    "        \"metamask\", \"mining\", \"mine\", \"javascript\", \"private blockchain\", \"wallet\", \"wallets\", \"gas\", \"parity\", \"parities\"]\n",
    "\n",
    "for i in tags:\n",
    "    count = 0\n",
    "    for key in post_data:\n",
    "        counter = Counter(key['content'].lower().split(\" \"))\n",
    "        count += counter[i]\n",
    "            # print(key['title'])\n",
    "    print(i, \"=\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300, 556, 295, 58, 29]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../conference_medium_data/author_data/related_data_rm_duplicacy.json\") as f:\n",
    "    post_data = json.load(f)\n",
    "    \n",
    "read_list = []\n",
    "for key in post_data:\n",
    "    read_list.append(key['readtime'])\n",
    "\n",
    "_0_2 = 0\n",
    "_2_5 = 0\n",
    "_5_10 = 0\n",
    "_10_15 = 0\n",
    "_15 = 0\n",
    "for i in read_list:\n",
    "    i = float(i)\n",
    "    if i<2:\n",
    "        _0_2 += 1\n",
    "    elif i>=2 and i<5:\n",
    "        _2_5 += 1\n",
    "    elif i>=5 and i<10:\n",
    "        _5_10 += 1\n",
    "    elif i>=10 and i<15:\n",
    "        _10_15 += 1\n",
    "    else:\n",
    "        _15 += 1\n",
    "\n",
    "read_count = [_0_2, _2_5, _5_10, _10_15, _15]\n",
    "read_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "round(1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.54"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '1.54'\n",
    "a = float(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
