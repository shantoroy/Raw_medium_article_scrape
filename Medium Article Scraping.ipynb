{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Medium articles with tags ML/DL/AI using BS & SELENIUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all libraries\n",
    "import urllib.request\n",
    "import requests\n",
    "import time\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the links we wanna scrape\n",
    "ethereum = \"https://medium.com/tag/ethereum/archive/2019/08/09\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_url = \"https://medium.com/tag/ethereum/archive\"\n",
    "\n",
    "year = 2018\n",
    "\n",
    "scrape_url_list = []\n",
    "while(year>=2014):\n",
    "    month=12\n",
    "    while(month>=1):\n",
    "        date=30\n",
    "        while(date>=1):\n",
    "            new_url=base_url + \"/\" + str(year) + \"/\" + str(month).zfill(2) + \"/\" + str(date).zfill(2)\n",
    "            scrape_url_list.append(new_url)\n",
    "            date-=1\n",
    "        month-=1\n",
    "    year-=1\n",
    "# len(scrape_url_list)    # the returned number is 2160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic loading with selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "driver = webdriver.Chrome('/home/mrx/Downloads/chromedriver')\n",
    "for i in scrape_url_list:\n",
    "    #driver.get(i)\n",
    "    #res = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "    #driver.quit()\n",
    "    page=urllib.request.Request(i,headers={'User-Agent': 'Mozilla/5.0'}) \n",
    "    response=urllib.request.urlopen(page).read()\n",
    "    f = getdata(response)\n",
    "    \n",
    "    \n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the webpage that is displayed till you reach the end to collect all tagged articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f62bae8c9d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0methereum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"return document.documentElement.outerHTML\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "driver.get(ethereum)\n",
    "res = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def getdata(res):\n",
    "    soup = BeautifulSoup(res, 'lxml')\n",
    "    work=[]\n",
    "    for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link--darken link--accent u-accentColor--textNormal\"):\n",
    "        work.append(my_tag.text)\n",
    "    para=[]\n",
    "    for my_tag in soup.find_all(True,{'class':['graf graf--h4 graf-after--h3 graf--trailing graf--subtitle','graf graf--p graf-after--figure','graf graf--p graf-after--h3 graf--trailing']}):\n",
    "        para.append(my_tag.text)\n",
    "    title=[]\n",
    "    for my_tag in soup.find_all(True,{'class':['graf graf--h3 graf-after--figure graf--title','graf graf--h3 graf--leading graf--title','graf graf--h3 graf-after--figure graf--trailing graf--title']}):\n",
    "        title.append(my_tag.text)\n",
    "    read_time=[]\n",
    "    for my_tag in soup.find_all(class_=\"readingTime\"):\n",
    "        read_time.append(my_tag.get('title'))\n",
    "    upvotes=[]\n",
    "    for my_tag in soup.find_all('span',{'class':'u-relative u-background js-actionMultirecommendCount u-marginLeft5'}):\n",
    "        upvotes.append(my_tag.text)\n",
    "    date=[]\n",
    "    for my_tag in soup.find_all('time'):\n",
    "        date.append(my_tag.text)\n",
    "    author_name = []\n",
    "    for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken\"):\n",
    "        author_name.append(my_tag.text)\n",
    "    content=[]\n",
    "    alllink=[]\n",
    "    for my_tag in soup.find_all(class_=\"postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls\"):\n",
    "        content.append(my_tag.text)\n",
    "        for link in my_tag.find_all('a'):\n",
    "            alllink.append(link.get('href'))\n",
    "    for i in range(0,len(content)):\n",
    "        content[i]=content[i].replace(author_name[i],'')\n",
    "        content[i]=content[i].replace(date[i],'')\n",
    "        content[i]=content[i].replace(upvotes[i],'')\n",
    "        content[i]=content[i].replace(title[i],'')\n",
    "        content[i]=content[i].replace('…Read more…','*')\n",
    "        content[i]=content[i].replace('Read more…','*')\n",
    "        content[i]=content[i].replace('\\xa0',' ')\n",
    "        content[i]=content[i].replace(' in ', '')\n",
    "        for tag in work:\n",
    "            content[i] = content[i].replace(tag, '')\n",
    "        content[i]=content[i].split('*')\n",
    "        \n",
    "    body=[]\n",
    "    for i in range(0,len(content)):\n",
    "        body.append(content[i][0])\n",
    "        \n",
    "    res =[]\n",
    "    for i in range(0,len(content)):\n",
    "        if (len(content[i]) == 1):\n",
    "            res.append('')\n",
    "        else:\n",
    "            res.append(content[i][1])\n",
    "            \n",
    "    links=[]\n",
    "    for my_tag in soup.find_all(class_=\"button button--smaller button--chromeless u-baseColor--buttonNormal\"):\n",
    "        links.append(my_tag.get('href'))\n",
    "        \n",
    "    data = pd.DataFrame({'1.Tag':'Ethereum','2.Name':author_name,'3.Title':title,'4.Body':body,'5.Upvotes':upvotes,'6.Date':date,'7.Comments':res,'8.Link':links}) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Articles tagged in Ethereum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chain',\n",
       " 'Hacker Noon',\n",
       " 'Hacker Noon',\n",
       " 'freeCodeCamp.org',\n",
       " 'Hacker Noon',\n",
       " 'Startup Grind']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input all blog sites\n",
    "work=[]\n",
    "for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link--darken link--accent u-accentColor--textNormal\"):\n",
    "    work.append(my_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input all the paragraphs\n",
    "para=[]\n",
    "for my_tag in soup.find_all(True,{'class':['graf graf--h4 graf-after--h3 graf--trailing graf--subtitle','graf graf--p graf-after--figure','graf graf--p graf-after--h3 graf--trailing']}):\n",
    "    para.append(my_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And anyone else still struggling to understand cryptocurrencies', 'Blockchain technology explained in simple\\xa0words', 'Trading cryptocurrencies for\\xa0dummies.', 'Ever since Nas Daily’s video came out about how I earned over $400,000 with less than $10,000 investing in Bitcoin and…', 'I was wrong about Ethereum because it’s such a good store of value…\\xa0no…']\n"
     ]
    }
   ],
   "source": [
    "len(para)\n",
    "print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "for my_tag in soup.find_all(True,{'class':['graf graf--h3 graf-after--figure graf--title','graf graf--h3 graf--leading graf--title','graf graf--h3 graf-after--figure graf--trailing graf--title']}):\n",
    "    title.append(my_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Letter to Jamie\\xa0Dimon',\n",
       " 'WTF is The Blockchain?',\n",
       " 'NEO versus Ethereum: Why NEO might be 2018’s strongest cryptocurrency',\n",
       " 'How Does the Blockchain Work?',\n",
       " 'A hacker stole $31M of Ether\\u200a—\\u200ahow it happened, and what it means for\\xa0Ethereum',\n",
       " '10 Step Guide for Day Trading Bitcoin, Ethereum and\\xa0Litecoin',\n",
       " 'Why Everyone Missed the Most Important Invention in the Last 500\\xa0Years',\n",
       " 'How does Ethereum work,\\xa0anyway?',\n",
       " 'Cryptocurrency 101',\n",
       " 'I was wrong about\\xa0Ethereum']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_time=[]\n",
    "for my_tag in soup.find_all(class_=\"readingTime\"):\n",
    "    read_time.append(my_tag.get('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18 min read',\n",
       " '16 min read',\n",
       " '25 min read',\n",
       " '15 min read',\n",
       " '16 min read',\n",
       " '16 min read',\n",
       " '19 min read',\n",
       " '33 min read',\n",
       " '75 min read',\n",
       " '9 min read']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input the number of votes, name, date and other information into seperate arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "upvotes=[]\n",
    "for my_tag in soup.find_all('span',{'class':'u-relative u-background js-actionMultirecommendCount u-marginLeft5'}):\n",
    "    upvotes.append(my_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "date=[]\n",
    "for my_tag in soup.find_all('time'):\n",
    "    date.append(my_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chain',\n",
       " 'Mohit Mamoria',\n",
       " 'Noam Levenson',\n",
       " \"Michele D'Aliessi\",\n",
       " 'Haseeb Qureshi',\n",
       " 'Kyle Hill',\n",
       " 'Daniel Jeffries',\n",
       " 'Preethi Kasireddy',\n",
       " 'Ben Yu',\n",
       " 'WhalePanda']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_name = []\n",
    "for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken\"):\n",
    "    author_name.append(my_tag.text)\n",
    "author_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "content=[]\n",
    "alllink=[]\n",
    "for my_tag in soup.find_all(class_=\"postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls\"):\n",
    "    content.append(my_tag.text)\n",
    "    for link in my_tag.find_all('a'):\n",
    "        alllink.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(content)):\n",
    "    content[i]=content[i].replace(author_name[i],'')\n",
    "    content[i]=content[i].replace(date[i],'')\n",
    "    content[i]=content[i].replace(upvotes[i],'')\n",
    "    content[i]=content[i].replace(title[i],'')\n",
    "    content[i]=content[i].replace('…Read more…','*')\n",
    "    content[i]=content[i].replace('Read more…','*')\n",
    "    content[i]=content[i].replace('\\xa0',' ')\n",
    "    content[i]=content[i].replace(' in ', '')\n",
    "    for tag in work:\n",
    "        content[i] = content[i].replace(tag, '')\n",
    "    content[i]=content[i].split('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7fe70820e08a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7ab2a7b7205a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    }
   ],
   "source": [
    "body=[]\n",
    "for i in range(0,len(content)):\n",
    "    body.append(content[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all arrays have an equal length of 50 (There are 50 articles loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "res =[]\n",
    "for i in range(0,len(content)):\n",
    "    if (len(content[i]) == 1):\n",
    "        res.append('')\n",
    "    else:\n",
    "        res.append(content[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accept the links for that redirects to the articles (Read more...portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=[]\n",
    "for my_tag in soup.find_all(class_=\"button button--smaller button--chromeless u-baseColor--buttonNormal\"):\n",
    "        links.append(my_tag.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the mising link in the 47th index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add missing link\n",
    "#links.insert(47,'https://medium.com/@matrixainetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all arrays into a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'1.Tag':'Ethereum','2.Name':author_name,'3.Title':title,'4.Body':body,'5.Upvotes':upvotes,'6.Date':date,'7.Comments':res,'8.Link':links}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Articles tagged in MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('C:/Users/Sangarshanan Veera/Desktop/chromedriver.exe')\n",
    "driver.get(machine_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(page):\n",
    "    work=[]\n",
    "    for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link--darken link--accent u-accentColor--textNormal\"):\n",
    "        work.append(my_tag.text)\n",
    "    para=[]\n",
    "    for my_tag in soup.find_all(True,{'class':['graf graf--h4 graf-after--h3 graf--trailing graf--subtitle','graf graf--p graf-after--figure','graf graf--p graf-after--h3 graf--trailing']}):\n",
    "        para.append(my_tag.text)\n",
    "    title=[]\n",
    "    for my_tag in soup.find_all(True,{'class':['graf graf--h3 graf-after--figure graf--title','graf graf--h3 graf--leading graf--title','graf graf--h3 graf-after--figure graf--trailing graf--title']}):\n",
    "        title.append(my_tag.text)\n",
    "    read_time=[]\n",
    "    for my_tag in soup.find_all(class_=\"readingTime\"):\n",
    "        read_time.append(my_tag.get('title'))\n",
    "    upvotes=[]\n",
    "    for my_tag in soup.find_all('span',{'class':'u-relative u-background js-actionMultirecommendCount u-marginLeft5'}):\n",
    "        upvotes.append(my_tag.text)\n",
    "    date=[]\n",
    "    for my_tag in soup.find_all('time'):\n",
    "        date.append(my_tag.text)\n",
    "    author_name = []\n",
    "    for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken\"):\n",
    "        author_name.append(my_tag.text)\n",
    "    content=[]\n",
    "    alllink=[]\n",
    "    for my_tag in soup.find_all(class_=\"postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls\"):\n",
    "        content.append(my_tag.text)\n",
    "        for link in my_tag.find_all('a'):\n",
    "            alllink.append(link.get('href'))\n",
    "    for i in range(0,len(content)):\n",
    "        content[i]=content[i].replace(author_name[i],'')\n",
    "        content[i]=content[i].replace(date[i],'')\n",
    "        content[i]=content[i].replace(upvotes[i],'')\n",
    "        content[i]=content[i].replace(title[i],'')\n",
    "        content[i]=content[i].replace('…Read more…','*')\n",
    "        content[i]=content[i].replace('Read more…','*')\n",
    "        content[i]=content[i].replace('\\xa0',' ')\n",
    "        content[i]=content[i].replace(' in ', '')\n",
    "        for tag in work:\n",
    "            content[i] = content[i].replace(tag, '')\n",
    "        content[i]=content[i].split('*')\n",
    "        \n",
    "    body=[]\n",
    "    for i in range(0,len(content)):\n",
    "        body.append(content[i][0])\n",
    "        \n",
    "    res =[]\n",
    "    for i in range(0,len(content)):\n",
    "        if (len(content[i]) == 1):\n",
    "            res.append('')\n",
    "        else:\n",
    "            res.append(content[i][1])\n",
    "            \n",
    "    links=[]\n",
    "    for my_tag in soup.find_all(class_=\"button button--smaller button--chromeless u-baseColor--buttonNormal\"):\n",
    "        links.append(my_tag.get('href'))\n",
    "        \n",
    "    data = pd.DataFrame({'1.Tag':'Ethereum','2.Name':author_name,'3.Title':title,'4.Body':body,'5.Upvotes':upvotes,'6.Date':date,'7.Comments':res,'8.Link':links}) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat the process again for the ML tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work=[]\n",
    "for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link--darken link--accent u-accentColor--textNormal\"):\n",
    "    work.append(my_tag.text)\n",
    "para=[]\n",
    "for my_tag in soup.find_all(True,{'class':['graf graf--h4 graf-after--h3 graf--trailing graf--subtitle','graf graf--p graf-after--figure','graf graf--p graf-after--h3 graf--trailing']}):\n",
    "    para.append(my_tag.text)\n",
    "title=[]\n",
    "for my_tag in soup.find_all(True,{'class':['graf graf--h3 graf-after--figure graf--title','graf graf--h3 graf--leading graf--title','graf graf--h3 graf-after--figure graf--trailing graf--title']}):\n",
    "    title.append(my_tag.text)\n",
    "read=[]\n",
    "for my_tag in soup.find_all(class_=\"readingTime\"):\n",
    "    read.append(my_tag.get('title'))\n",
    "upvotes=[]\n",
    "for my_tag in soup.find_all('span',{'class':'u-relative u-background js-actionMultirecommendCount u-marginLeft5'}):\n",
    "    upvotes.append(my_tag.text)\n",
    "date=[]\n",
    "for my_tag in soup.find_all('time'):\n",
    "    date.append(my_tag.text)\n",
    "name = []\n",
    "for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken\"):\n",
    "    name.append(my_tag.text)\n",
    "content=[]\n",
    "alllink=[]\n",
    "for my_tag in soup.find_all(class_=\"postArticle postArticle--short js-postArticle js-trackPostPresentation js-trackPostScrolls\"):\n",
    "        content.append(my_tag.text)\n",
    "        for link in my_tag.find_all('a'):\n",
    "            alllink.append(link.get('href'))\n",
    "for i in range(0,len(content)):\n",
    "    content[i]=content[i].replace(name[i],'')\n",
    "    content[i]=content[i].replace(date[i],'')\n",
    "    content[i]=content[i].replace(upvotes[i],'')\n",
    "    content[i]=content[i].replace(title[i],'')\n",
    "    content[i]=content[i].replace('…Read more…','*')\n",
    "    content[i]=content[i].replace('Read more…','*')\n",
    "    content[i]=content[i].replace('\\xa0',' ')\n",
    "    content[i]=content[i].replace(' in ', '')\n",
    "    for tag in work:\n",
    "        content[i] = content[i].replace(tag, '')\n",
    "    content[i]=content[i].split('*')\n",
    "body=[]\n",
    "for i in range(0,len(content)):\n",
    "    body.append(content[i][0])\n",
    "\n",
    "res =[]\n",
    "for i in range(0,len(content)):\n",
    "    if (len(content[i]) == 1):\n",
    "        res.append('')\n",
    "    else:\n",
    "        res.append(content[i][1])\n",
    "\n",
    "links=[]\n",
    "for my_tag in soup.find_all(class_=\"button button--smaller button--chromeless u-baseColor--buttonNormal\"):\n",
    "        links.append(my_tag.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No length issues...so directly include into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.DataFrame({'1.Tag':'Machine Learning','2.Name':name,'3.Title':title,'4.Body':body,'5.Upvotes':upvotes,'6.Date':date,'7.Comments':res,'8.Link':links}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Articles tagged in DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('C:/Users/Sangarshanan Veera/Desktop/chromedriver.exe')\n",
    "driver.get(deep_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work=[]\n",
    "for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link--darken link--accent u-accentColor--textNormal\"):\n",
    "    work.append(my_tag.text)\n",
    "para=[]\n",
    "for my_tag in soup.find_all(True,{'class':['graf graf--h4 graf-after--h3 graf--trailing graf--subtitle','graf graf--p graf-after--figure','graf graf--p graf-after--h3 graf--trailing']}):\n",
    "    para.append(my_tag.text)\n",
    "title=[]\n",
    "for my_tag in soup.find_all(True,{'class':['graf graf--h3 graf-after--figure graf--title','graf graf--h3 graf--leading graf--title','graf graf--h3 graf-after--figure graf--trailing graf--title']}):\n",
    "    title.append(my_tag.text)\n",
    "read=[]\n",
    "for my_tag in soup.find_all(class_=\"readingTime\"):\n",
    "    read.append(my_tag.get('title'))\n",
    "upvotes=[]\n",
    "for my_tag in soup.find_all('span',{'class':'u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft5'}):\n",
    "    upvotes.append(my_tag.text)\n",
    "date=[]\n",
    "for my_tag in soup.find_all('time'):\n",
    "    date.append(my_tag.text)\n",
    "name = []\n",
    "for my_tag in soup.find_all(class_=\"ds-link ds-link--styleSubtle link link--darken link--accent u-accentColor--textNormal u-accentColor--textDarken\"):\n",
    "    name.append(my_tag.text)\n",
    "content=[]\n",
    "alllink=[]\n",
    "for my_tag in soup.find_all(class_=\"postArticle postArticle--short js-postArticle js-trackedPost\"):\n",
    "        content.append(my_tag.text)\n",
    "        for link in my_tag.find_all('a'):\n",
    "            alllink.append(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the missing title in the 33rd index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title.insert(33,'“WTH does a neural network even learn?” — A newcomer’s dilemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(content)):\n",
    "    content[i]=content[i].replace(name[i],'')\n",
    "    content[i]=content[i].replace(date[i],'')\n",
    "    content[i]=content[i].replace(upvotes[i],'')\n",
    "    content[i]=content[i].replace(title[i],'')\n",
    "    content[i]=content[i].replace('…Read more…','*')\n",
    "    content[i]=content[i].replace('Read more…','*')\n",
    "    content[i]=content[i].replace('\\xa0',' ')\n",
    "    content[i]=content[i].replace(' in ', '')\n",
    "    for tag in work:\n",
    "        content[i] = content[i].replace(tag, '')\n",
    "    content[i]=content[i].split('*')\n",
    "body=[]\n",
    "for i in range(0,len(content)):\n",
    "    body.append(content[i][0])\n",
    "\n",
    "res =[]\n",
    "for i in range(0,len(content)):\n",
    "    if (len(content[i]) == 1):\n",
    "        res.append('')\n",
    "    else:\n",
    "        res.append(content[i][1])\n",
    "\n",
    "links=[]\n",
    "for my_tag in soup.find_all(class_=\"button button--smaller button--chromeless u-baseColor--buttonNormal\"):\n",
    "        links.append(my_tag.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame({'1.Tag':'Deep Learning','2.Name':name,'3.Title':title,'4.Body':body,'5.Upvotes':upvotes,'6.Date':date,'7.Comments':res,'8.Link':links}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all three tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = data.append(data1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = f.append(data2,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.to_csv('medium.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('medium.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
